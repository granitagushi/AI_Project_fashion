{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Image Classification\n",
    "\n",
    "We use Hugging Face’s `pipeline` to load a CLIP model for zero-shot image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"openai/clip-vit-large-patch14\"\n",
    "detector = pipeline(model=checkpoint, task=\"zero-shot-image-classification\")\n",
    "#checkpoint = \"google/siglip-so400m-patch14-384\"\n",
    "#detector = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-so400m-patch14-384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Fashion-MNIST Dataset\n",
    "\n",
    "We load Zalando’s Fashion-MNIST from the Hugging Face Hub using the `datasets` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('zalando-datasets/fashion_mnist')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying a Single Example\n",
    "\n",
    "You can render the raw PIL image for the first training sample directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tbw1oNx4m8QWmkWx2yXD4LkZCADJJ+gFbviL4a63oc7COE3MW4hdn38duD976jNc9daDqllIsc9lKrMu4YGeMkdR7gj8KzcV7H8BtEvV16+1iWCeG1Wz8mOV02pIzupwCeuAp6Z98cZ90aIzLIlw0c0ZJ4KgjHoeOa+evjS9n/wnMcNxBPCYLKONFhA2FNzMpGenDcgd816V4K03wefC+m3NlpVhP+5QSXBiR5fMx825iMg5zwce3FdbOzTwgW90lu6uCm8eYrL02soIyCPQgggEdMGQ3cluiPNK0rJwrRQBNueuMkt+teNfGKxsdY8WWdxNqcNo66eieXMwVsb5DnH415Hp2rajpE5n02/urOUjBe3laMkehIPIrVm8eeLrhNknibVivoLtx/I1UPinxC3XXtUP1vJP8ay5JZJpGkldnduSzHJP41//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACD0lEQVR4AbWRz2sTQRTH38zszm42k6Q2TU2ixURstRdRasWC9FCsUE9SpIKnKl48+x94UOjNiyf/h4KgKN7TYgNKK+agjZS0wZDYbND90dn54VqxIWfxXebBB97nfecB/P9CAKkFAGT8ViEEh+9fLZZn7gde+E4AwkgAGYBEzl3btZz55y0tgSl/AHKYLhH85uJKdat2ebqyFmuOCun5laFIwcYXjvLRxq1nfRh3er0ESHAI1fvPYqF8oj9WxxO6hcAyWZhQV2fw6OvBbcEh2O/tlxTCjlRjgPtGAqwYcpN7GWUlPbppXepDTeB2AduQHONWpOzsi09GHxocPobUccZPh8RhcvfOy/V4IUQwigMIgFdeQHWb2BFEipzvxU6iBT9QALNPq8F3oYTnSRt8oN4iHOYcLk4UFs+GOEo0TZrlToXNqp7ZmkQw8yg3JIkrHI6C2lI1dawE9dQPP8HSDiJrRSF9IAFAZmT5+oNm+LU+nuVmispT6N6TbcYsMDONZg7nb9rl5NQU5pgCMq8YrUY6bDCa3t9hQShWt0rD3I0kNxWiE8aebiRH3E7bsEw7hTuTXqNrdSIRJfK9C8aH1bvNesioTcmB1P43JY2QcdeNRLkVR7nx8HjblYQaBGnTpCYC1AKq8ptLCMf55x6PZjAxJGrpPfWTgI58/LZW+fMJ8WXO5bond/j20Y3+sfkFaCTYdrBYeB0AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Candidate Labels\n",
    "\n",
    "We list the 10 Fashion-MNIST classes as strings for CLIP’s zero-shot pipeline, and prepare containers to hold true vs. predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "labels_fashion_mnist= [\n",
    "    'T - shirt / top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle boot'\n",
    "]\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/60000 [00:01<1:27:55, 11.37it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 60000/60000 [36:17<00:00, 27.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for example in tqdm(dataset['train']):\n",
    "    # image ist bereits ein PIL-Image\n",
    "    image = example['image']\n",
    "\n",
    "\n",
    "    # Zero-Shot-Vorhersage\n",
    "    results = detector(image, candidate_labels=labels_fashion_mnist)\n",
    "    sorted_results = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "    pred_label = sorted_results[0][\"label\"]\n",
    "\n",
    "    true_labels.append(example[\"label\"])\n",
    "    predicted_labels.append(pred_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Zero-Shot Classification Performance\n",
    "\n",
    "We convert the ground-truth integer labels into their string names, then use `sklearn.metrics` to compute accuracy, precision, and recall (weighted over all 10 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6097\n",
      "Precision: 0.5647\n",
      "Recall   : 0.6097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# 1) True-Labels von Integer → String konvertieren\n",
    "true_labels_str = [labels_fashion_mnist[i] for i in true_labels]\n",
    "\n",
    "# 2) Accuracy berechnen\n",
    "accuracy = accuracy_score(true_labels_str, predicted_labels)\n",
    "\n",
    "# 3) Precision und Recall berechnen (nur Strings in `labels=` übergeben)\n",
    "precision = precision_score(\n",
    "    true_labels_str,\n",
    "    predicted_labels,\n",
    "    average='weighted',\n",
    "    labels=labels_fashion_mnist\n",
    ")\n",
    "recall = recall_score(\n",
    "    true_labels_str,\n",
    "    predicted_labels,\n",
    "    average='weighted',\n",
    "    labels=labels_fashion_mnist\n",
    ")\n",
    "\n",
    "# 4) Ergebnisse ausgeben\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0553aaa676d48158c521bda20e5eb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66323e54bc14d01b6da26cc48ad9cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ca971c843848d2b8cd91fd8ce4ddec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/351 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://f20bea4282b020af30.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f20bea4282b020af30.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1) Models laden\n",
    "vit_classifier = pipeline(\n",
    "    task=\"image-classification\",\n",
    "    model=\"Granitagushi/vit-base-fashion\"\n",
    ")\n",
    "clip_detector = pipeline(\n",
    "    task=\"zero-shot-image-classification\",\n",
    "    model=\"openai/clip-vit-large-patch14\"\n",
    ")\n",
    "\n",
    "labels = [\n",
    "    'T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "# 2) Callback mit str-Casting\n",
    "def classify_clothing(image):\n",
    "    vit_results  = vit_classifier(image)\n",
    "    vit_output   = { str(r[\"label\"]): float(r[\"score\"]) for r in vit_results }\n",
    "\n",
    "    clip_results = clip_detector(image, candidate_labels=labels)\n",
    "    clip_output  = { str(r[\"label\"]): float(r[\"score\"]) for r in clip_results }\n",
    "\n",
    "    return {\n",
    "        \"ViT Classification\": vit_output,\n",
    "        \"CLIP Zero-Shot Classification\": clip_output\n",
    "    }\n",
    "\n",
    "# 3) Beispiele nur, wenn die Dateien ins Repo wandern\n",
    "example_images = [\n",
    "    [\"example_images/jeans.jpg\"],\n",
    "    [\"example_images/kleid.jpg\"],\n",
    "    [\"example_images/pullover.jpg\"],\n",
    "    [\"example_images/sandale.jpg\"],\n",
    "    [\"example_images/tasche.jpg\"]\n",
    "]\n",
    "\n",
    "# 4) Interface – cache_examples im Konstruktor\n",
    "iface = gr.Interface(\n",
    "    fn=classify_clothing,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload ein Fashion-MNIST-Bild\"),\n",
    "    outputs=gr.JSON(label=\"Ergebnisse\"),\n",
    "    title=\"Fashion MNIST Klassifikation\",\n",
    "    description=\"Vergleiche dein ViT-Modell mit einem CLIP Zero-Shot-Modell.\",\n",
    "    examples=example_images,    # <— nur, wenn Dateien wirklich da sind\n",
    "    cache_examples=False\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
